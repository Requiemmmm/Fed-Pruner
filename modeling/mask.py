import math
import torch
import torch.nn as nn
import torch.nn.functional as F
import numpy as np
from torch import Tensor
from typing import Union, Tuple, Optional
import logging

logger = logging.getLogger(__name__)


def sample(inps: Union[Tuple[torch.Tensor, ...], torch.Tensor], size: int):
    if isinstance(inps, torch.Tensor):
        assert len(inps.shape) == 3 and inps.shape[0] == 1
        inps = inps.squeeze(0).cpu()  # (seq_length, hidden_size)
        size = min(inps.shape[0], size)
        indices = np.random.choice(inps.shape[0], size, replace=False)
        indices = torch.from_numpy(indices)
        return inps[indices]
    else:
        return tuple(sample(x, size) for x in inps)


class Mask(nn.Module):
    """
    ä¿®å¤åŽçš„æŽ©ç ç±»ï¼š
    1. ä¿®æ­£åˆå§‹åŒ–ç­–ç•¥ï¼Œä»Žä½Žç¨€ç–åº¦å¼€å§‹
    2. å¢žå¼ºæ•°å€¼ç¨³å®šæ€§
    3. æ·»åŠ è°ƒè¯•ä¿¡æ¯
    4. æ”¹è¿›ç¨€ç–åº¦æŽ§åˆ¶
    """
    min_s = -0.1
    max_s = 1.1
    eps = 1e-6
    magical_number = 0.8
    
    def __init__(self, features: int, repeat: int = 1) -> None:
        super().__init__()
        self.activate = nn.Parameter(torch.tensor(True), requires_grad=False)
        self.features = features * repeat
        self.repeat = repeat
        self.beta = 2. / 3.
        self.log_alpha = nn.Parameter(torch.zeros((features,)))
        self.sampler = torch.distributions.Uniform(self.eps, 1. - self.eps)
        
        # ðŸ”§ ä¿®å¤1: ä»Žä½Žç¨€ç–åº¦å¼€å§‹ï¼Œé€æ¸å¢žåŠ å‰ªæž
        # åŽŸæ¥: self.set_params(10.0) -> L()â‰ˆ1.0 (å‡ ä¹Žä¸å‰ªæž)
        # ä¿®æ­£: ä»Ž-2.0å¼€å§‹ï¼Œå¯¹åº”çº¦30%çš„ä¿ç•™çŽ‡ï¼Œéšè®­ç»ƒé€æ¸å‰ªæž
        self.set_params(-2.0)  # å¯¹åº”åˆå§‹L()â‰ˆ0.3ï¼Œå¼€å§‹å°±æœ‰ä¸€å®šå‰ªæž
        
        # æ·»åŠ è°ƒè¯•è®¡æ•°å™¨
        self.debug_call_count = 0
        
    def set_state(self, activate: bool):
        self.activate.copy_(activate)
    
    @torch.no_grad()
    def set_params(self, mean: float, indices: Optional[torch.LongTensor] = None):
        """
        ðŸ”§ ä¿®å¤2: å¢žå¼ºå‚æ•°è®¾ç½®çš„æ•°å€¼ç¨³å®šæ€§
        """
        # é™åˆ¶meançš„èŒƒå›´ï¼Œé¿å…æžç«¯å€¼
        mean_tensor = torch.tensor(mean)
        mean = torch.clamp(mean_tensor, min=-10.0, max=10.0).item()
        
        if indices is None:
            self.log_alpha.normal_(mean=mean, std=1e-2)
        else:
            self.log_alpha[indices].normal_(mean=mean, std=1e-2)
        
        # é™åˆ¶log_alphaèŒƒå›´ï¼Œç¡®ä¿æ•°å€¼ç¨³å®šæ€§
        self.log_alpha.data = torch.clamp(self.log_alpha.data, min=-15.0, max=15.0)
    
    def L(self):
        """
        ðŸ”§ ä¿®å¤3: å¢žå¼ºL()è®¡ç®—çš„æ•°å€¼ç¨³å®šæ€§å¹¶æ·»åŠ è°ƒè¯•ä¿¡æ¯
        """
        log_alpha = self.log_alpha.repeat(self.repeat)
        
        # è®¡ç®—logitsï¼Œæ·»åŠ æ•°å€¼ç¨³å®šæ€§æ£€æŸ¥
        x = (0 - self.min_s) / (self.max_s - self.min_s)
        x = torch.clamp(torch.tensor(x), min=self.eps, max=1 - self.eps)  # ç¡®ä¿xåœ¨æœ‰æ•ˆèŒƒå›´å†…
        logits = math.log(x.item()) - math.log(1 - x.item())
        
        # æ·»åŠ æ•°å€¼ç¨³å®šæ€§æ£€æŸ¥
        alpha_term = log_alpha - logits * self.beta
        alpha_term = torch.clamp(alpha_term, min=-15.0, max=15.0)  # é˜²æ­¢sigmoidæº¢å‡º
        
        L = torch.sigmoid(alpha_term).clamp(min=self.eps, max=1-self.eps)
        
        # è°ƒè¯•ä¿¡æ¯ï¼ˆæ¯100æ¬¡è°ƒç”¨æ‰“å°ä¸€æ¬¡ï¼‰
        self.debug_call_count += 1
        if self.debug_call_count % 100 == 0:
            expected_retention = L.mean().item()
            expected_sparsity = 1.0 - expected_retention
            logger.debug(f"Mask L() - Expected retention: {expected_retention:.4f}, "
                        f"Expected sparsity: {expected_sparsity:.4f}, "
                        f"Features: {self.features//self.repeat}")
        
        if not self.activate.item():
            L = L.detach()
        return L

    def sample_z(self):
        """
        ðŸ”§ ä¿®å¤4: å¢žå¼ºé‡‡æ ·çš„æ•°å€¼ç¨³å®šæ€§
        """
        log_alpha = self.log_alpha.repeat(self.repeat)
        u = self.sampler.sample((self.features,)).type_as(log_alpha)
        
        # æ·»åŠ æ•°å€¼ç¨³å®šæ€§
        log_u = torch.log(u)
        log_1_minus_u = torch.log(1 - u)
        
        # é˜²æ­¢é™¤é›¶å’Œæº¢å‡º
        alpha_term = (log_u - log_1_minus_u + log_alpha) / self.beta
        alpha_term = torch.clamp(alpha_term, min=-15.0, max=15.0)
        
        s = torch.sigmoid(alpha_term)
        s_bar = s * (self.max_s - self.min_s) + self.min_s
        z = F.hardtanh(s_bar, min_val=0, max_val=1)
        return z
    
    def deterministic_z(self):
        """
        ðŸ”§ ä¿®å¤5: æ”¹è¿›ç¡®å®šæ€§æŽ©ç ç”Ÿæˆé€»è¾‘
        """
        sub_features = self.features // self.repeat
        Lc = self.L().sum() / self.repeat
        
        # ðŸ”§ å…³é”®ä¿®å¤ï¼šæ­£ç¡®è®¡ç®—è¦ç½®é›¶çš„å‚æ•°æ•°é‡
        # åŽŸé€»è¾‘ï¼šnum_zeros = round(sub_features - Lc.item()) * self.repeat
        # é—®é¢˜ï¼šå½“LcæŽ¥è¿‘sub_featuresæ—¶ï¼Œnum_zerosæŽ¥è¿‘0ï¼Œå‡ ä¹Žä¸å‰ªæž
        # ä¿®æ­£ï¼šåŸºäºŽç›®æ ‡ç¨€ç–åº¦è®¡ç®—
        
        target_retention_rate = Lc.item() / sub_features  # Lçš„å¹³å‡å€¼å°±æ˜¯ç›®æ ‡ä¿ç•™çŽ‡
        num_to_keep = max(1, round(sub_features * target_retention_rate))  # è‡³å°‘ä¿ç•™1ä¸ª
        num_zeros = sub_features - num_to_keep
        
        log_alpha = self.log_alpha.repeat(self.repeat)
        z = torch.sigmoid(log_alpha / self.beta * self.magical_number)
        
        if num_zeros > 0 and num_zeros < self.features:
            # é€‰æ‹©æœ€å°çš„num_zerosä¸ªå…ƒç´ ç½®é›¶
            _, indices = torch.topk(z, k=num_zeros, largest=False)
            z_new = z.clone()
            z_new[indices] = 0
            z = z_new
        
        # è°ƒè¯•ä¿¡æ¯
        if self.debug_call_count % 100 == 0:
            actual_zeros = (z == 0).sum().item()
            actual_sparsity = actual_zeros / self.features
            logger.debug(f"Deterministic mask - Target retention: {target_retention_rate:.4f}, "
                        f"Actual sparsity: {actual_sparsity:.4f}, "
                        f"Zeros: {actual_zeros}/{self.features}")
        
        return z
    
    def forward(self):
        if self.activate.item():
            if self.training:
                return self.sample_z()
            else:
                return self.deterministic_z()
        else:
            return self.deterministic_z().detach()

    @torch.no_grad()
    def parse(self):
        """
        ðŸ”§ ä¿®å¤6: æ”¹è¿›æŽ©ç è§£æžé€»è¾‘
        """
        sub_features = self.features // self.repeat
        Lc = self.L().sum() / self.repeat
        
        # æ›´ç¨³å¥çš„è®¡ç®—ä¿ç•™å…ƒç´ æ•°é‡
        target_retention_rate = Lc.item() / sub_features
        num_non_zeros = max(1, round(sub_features * target_retention_rate))
        num_non_zeros = min(num_non_zeros, sub_features)  # ä¸èƒ½è¶…è¿‡æ€»æ•°
        
        z = torch.sigmoid(self.log_alpha / self.beta * self.magical_number)
        
        # é€‰æ‹©top-kä¸ªæœ€å¤§çš„å…ƒç´ ä½œä¸ºä¿ç•™çš„
        indices = torch.topk(z, k=num_non_zeros).indices
        indices = torch.sort(indices).values
        
        if self.repeat > 1:  # shape: (num_heads, head_dim)
            z = z.repeat(self.repeat)
            indices = torch.concat(tuple(indices + i * sub_features for i in range(self.repeat)))
        
        return z[indices], indices

    def get_effective_sparsity(self):
        """
        ðŸ”§ æ–°å¢ž: èŽ·å–å½“å‰æŽ©ç çš„å®žé™…ç¨€ç–åº¦
        """
        with torch.no_grad():
            current_mask = self.deterministic_z()
            zeros = (current_mask == 0).sum().item()
            total = current_mask.numel()
            sparsity = zeros / total
            return sparsity, zeros, total

    def update_sparsity_target(self, target_sparsity: float):
        """
        ðŸ”§ æ–°å¢ž: æ ¹æ®ç›®æ ‡ç¨€ç–åº¦è°ƒæ•´log_alpha
        """
        # æ ¹æ®ç›®æ ‡ç¨€ç–åº¦åæŽ¨éœ€è¦çš„log_alphaå€¼
        # target_sparsity = 1 - retention_rate
        target_retention = 1.0 - target_sparsity
        target_retention = torch.clamp(torch.tensor(target_retention), min=0.01, max=0.99)
        
        # é€šè¿‡sigmoidåå‡½æ•°è®¡ç®—éœ€è¦çš„log_alpha
        # retention â‰ˆ sigmoid(log_alpha), æ‰€ä»¥ log_alpha â‰ˆ logit(retention)
        target_logit = torch.log(target_retention / (1 - target_retention))
        
        # å¹³æ»‘æ›´æ–°log_alpha
        with torch.no_grad():
            current_alpha = self.log_alpha.mean()
            # ä½¿ç”¨æŒ‡æ•°ç§»åŠ¨å¹³å‡å¹³æ»‘æ›´æ–°
            momentum = 0.9
            new_alpha = momentum * current_alpha + (1 - momentum) * target_logit
            self.log_alpha.data.fill_(new_alpha.item())
            
            # æ·»åŠ å°é‡éšæœºæ‰°åŠ¨ï¼Œé¿å…æ‰€æœ‰å‚æ•°å®Œå…¨ä¸€è‡´
            noise = torch.randn_like(self.log_alpha) * 0.01
            self.log_alpha.data += noise


class LinearWithMaskBefore(nn.Linear):
    
    def __init__(self, 
        in_features: int, 
        out_features: int, 
        bias: bool = True, 
        device=None, 
        dtype=None,
    ) -> None:
        super().__init__(in_features, out_features, bias, device, dtype)
        self.mask = Mask(in_features)
        self.kw_args = {
            "in_features": in_features,
            "out_features": out_features,
            "bias": bias,
            "device": device, 
            "dtype": dtype,
        }

    def super_forward(self, x: Tensor) -> Tensor:
        return super().forward(x)
    
    def forward(self, x: Tensor) -> Tensor:
        # ðŸ”§ ä¿®å¤7: ç¡®ä¿æŽ©ç ç¡®å®žç”Ÿæ•ˆ
        mask_values = self.mask()
        
        # è°ƒè¯•ï¼šéªŒè¯æŽ©ç æ˜¯å¦çœŸçš„åœ¨å·¥ä½œ
        if hasattr(self, '_debug_counter'):
            self._debug_counter += 1
        else:
            self._debug_counter = 1
            
        if self._debug_counter % 1000 == 0:  # æ¯1000æ¬¡å‰å‘ä¼ æ’­æ‰“å°ä¸€æ¬¡
            zeros = (mask_values == 0).sum().item()
            total = mask_values.numel()
            logger.debug(f"LinearWithMaskBefore - Masked {zeros}/{total} parameters "
                        f"({zeros/total*100:.1f}% pruned)")
        
        x = x * mask_values
        x = super().forward(x)
        return x

    def get_hook_fn(self, act_dict, name):
        def fn(module, inp, outp):
            assert len(inp[0].shape) == 3 and inp[0].shape[0] == 1
            inp: Tensor = inp[0].squeeze(0).cpu()
            act_values = inp.abs().max(dim=0).values
            filter_weights = module.weight.norm(dim=0)
            values = act_values * filter_weights
            if name not in act_dict:
                act_dict[name] = values
            else:
                act_dict[name] = torch.max(act_dict[name], values)            
        return fn

    def extract(self,
        indices: torch.Tensor,
        values: Optional[torch.Tensor] = None,
    ) -> nn.Linear:
        if values is None:
            values = torch.ones_like(indices, dtype=self.weight.dtype)
        values = torch.diag(values)

        self.kw_args["in_features"] = indices.shape[0]
        new_linear = nn.Linear(**self.kw_args)        
        new_linear.weight.copy_(self.weight[:, indices] @ values)
        if new_linear.bias is not None:
            new_linear.bias.copy_(self.bias)        
        return new_linear
