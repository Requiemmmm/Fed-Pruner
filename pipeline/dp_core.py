# pipeline/dp_core.py
# Fed-Prunerå·®åˆ†éšç§æ ¸å¿ƒå®ç°
# åŸºäºDP-FedAvgç®—æ³•çš„ä¸¥æ ¼(Îµ,Î´)-å·®åˆ†éšç§ä¿è¯

import torch
import torch.nn as nn
import numpy as np
import math
import logging
import time
from typing import Dict, List, Tuple, Optional, Union, Any
from collections import OrderedDict
from copy import deepcopy
from dataclasses import dataclass
from enum import Enum

logger = logging.getLogger(__name__)


@dataclass
class DPConfig:
    """å·®åˆ†éšç§é…ç½®ç±»"""
    # æ ¸å¿ƒDPå‚æ•°
    target_epsilon: float = 10.0  # ç›®æ ‡éšç§é¢„ç®—
    target_delta: float = 1e-5  # ç›®æ ‡å¤±è´¥æ¦‚ç‡
    noise_multiplier: float = 1.0  # å™ªå£°ä¹˜æ•°
    clipping_bound: float = 1.0  # æ¢¯åº¦å‰ªè£ç•Œé™

    # è®­ç»ƒå‚æ•°
    num_epochs: int = 10  # æ€»è®­ç»ƒè½®æ•°
    num_clients: int = 10  # å®¢æˆ·ç«¯æ•°é‡
    sampling_rate: float = 1.0  # å®¢æˆ·ç«¯é‡‡æ ·ç‡

    # é«˜çº§è®¾ç½®
    accountant_type: str = "rdp"  # éšç§ä¼šè®¡ç±»å‹: "rdp", "gdp"
    auto_clip: bool = True  # è‡ªåŠ¨å‰ªè£é˜ˆå€¼è°ƒæ•´
    adaptive_noise: bool = True  # è‡ªé€‚åº”å™ªå£°è°ƒæ•´

    def validate(self):
        """éªŒè¯é…ç½®æœ‰æ•ˆæ€§"""
        assert self.target_epsilon > 0, "target_epsilon must be positive"
        assert 0 < self.target_delta < 1, "target_delta must be in (0,1)"
        assert self.noise_multiplier > 0, "noise_multiplier must be positive"
        assert self.clipping_bound > 0, "clipping_bound must be positive"
        assert 0 < self.sampling_rate <= 1, "sampling_rate must be in (0,1]"


class PrivacyAccountant:
    """é«˜ç²¾åº¦éšç§ä¼šè®¡ç³»ç»Ÿï¼Œæ”¯æŒRDPå’ŒGDP"""

    def __init__(self, delta: float = 1e-5, accountant_type: str = "rdp"):
        self.delta = delta
        self.accountant_type = accountant_type.lower()

        if self.accountant_type == "rdp":
            # RDP orders from 1.25 to 64 for high precision
            self.orders = np.concatenate([
                np.linspace(1.25, 2, 16),
                np.logspace(np.log10(2), np.log10(64), 50)
            ])
            self.rdp_eps = np.zeros_like(self.orders)

        self.steps = 0
        self.history = []

    def step(self, noise_multiplier: float, sampling_rate: float,
             num_selected: int = None) -> float:
        """è®°å½•ä¸€æ­¥è®­ç»ƒçš„éšç§æ¶ˆè€—"""
        self.steps += 1

        if self.accountant_type == "rdp":
            eps = self._rdp_step(noise_multiplier, sampling_rate, num_selected)
        else:
            eps = self._gdp_step(noise_multiplier, sampling_rate, num_selected)

        self.history.append({
            'step': self.steps,
            'epsilon': eps,
            'noise_multiplier': noise_multiplier,
            'sampling_rate': sampling_rate
        })

        return eps

    def _rdp_step(self, noise_multiplier: float, sampling_rate: float,
                  num_selected: int = None) -> float:
        """RDPéšç§ä¼šè®¡"""
        if noise_multiplier <= 0:
            return float('inf')

        # è®¡ç®—æ¯ä¸ªorderçš„RDPå€¼
        for i, order in enumerate(self.orders):
            if sampling_rate == 1.0:
                # æ— é‡‡æ ·æƒ…å†µ
                rdp = order / (2 * noise_multiplier ** 2)
            else:
                # æ³Šæ¾é‡‡æ ·æƒ…å†µä¸‹çš„RDP
                rdp = self._compute_rdp_poisson_sampling(
                    order, noise_multiplier, sampling_rate
                )

            self.rdp_eps[i] += rdp

        # è½¬æ¢ä¸º(Îµ,Î´)-DP
        return self._rdp_to_dp()

    def _compute_rdp_poisson_sampling(self, order: float, noise_multiplier: float,
                                      sampling_rate: float) -> float:
        """è®¡ç®—æ³Šæ¾é‡‡æ ·ä¸‹çš„RDPå€¼"""
        if sampling_rate >= 1.0:
            return order / (2 * noise_multiplier ** 2)

        alpha = order
        p = sampling_rate
        sigma = noise_multiplier

        if alpha == 1:
            return p * (np.exp(1 / (2 * sigma ** 2)) - 1)

        # For Î± > 1, use the tighter bound
        log_a = (alpha - 1) * np.log(1 - p + p * np.exp(1 / sigma ** 2)) - np.log(alpha)
        a = np.exp(log_a)

        if a <= 0:
            return 0

        return np.log(a) / (alpha - 1)

    def _rdp_to_dp(self) -> float:
        """å°†RDPè½¬æ¢ä¸º(Îµ,Î´)-DP"""
        min_eps = float('inf')

        for i, order in enumerate(self.orders):
            if self.rdp_eps[i] == 0:
                continue

            eps = self.rdp_eps[i] + np.log(1 / self.delta) / (order - 1)
            min_eps = min(min_eps, eps)

        return min_eps if min_eps < float('inf') else 0.0

    def _gdp_step(self, noise_multiplier: float, sampling_rate: float,
                  num_selected: int = None) -> float:
        """GDP (Gaussian Differential Privacy) ä¼šè®¡"""
        sigma = noise_multiplier
        mu = np.sqrt(2 * np.log(1.25 / self.delta)) / sigma

        # ç´¯ç§¯éšç§æŸå¤±
        if not hasattr(self, 'gdp_mu'):
            self.gdp_mu = 0.0

        self.gdp_mu += mu * sampling_rate

        # è½¬æ¢ä¸ºÎµ
        eps = self.gdp_mu + np.sqrt(2 * self.gdp_mu * np.log(1 / self.delta))
        return eps

    def get_privacy_spent(self) -> Tuple[float, float]:
        """è·å–å·²æ¶ˆè€—çš„éšç§é¢„ç®—"""
        if self.steps == 0:
            return 0.0, self.delta
        return self.history[-1]['epsilon'], self.delta

    def reset(self):
        """é‡ç½®ä¼šè®¡"""
        if self.accountant_type == "rdp":
            self.rdp_eps.fill(0)
        else:
            self.gdp_mu = 0.0
        self.steps = 0
        self.history.clear()


class AdaptiveClipping:
    """è‡ªé€‚åº”å‰ªè£é˜ˆå€¼è°ƒæ•´"""

    def __init__(self, initial_bound: float, target_quantile: float = 0.5):
        self.initial_bound = initial_bound
        self.current_bound = initial_bound
        self.target_quantile = target_quantile
        self.norm_history = []
        self.update_frequency = 50  # æ¯50ä¸ªæ›´æ–°è°ƒæ•´ä¸€æ¬¡

    def update(self, norm: float):
        """æ›´æ–°å‰ªè£é˜ˆå€¼"""
        self.norm_history.append(norm)

        if len(self.norm_history) >= self.update_frequency:
            # è®¡ç®—ç›®æ ‡åˆ†ä½æ•°
            new_bound = np.quantile(self.norm_history, self.target_quantile)

            # å¹³æ»‘æ›´æ–°
            momentum = 0.9
            self.current_bound = momentum * self.current_bound + (1 - momentum) * new_bound

            # æ¸…ç©ºå†å²
            self.norm_history = []

            logger.debug(f"Adaptive clipping: updated bound to {self.current_bound:.4f}")

    def get_threshold(self) -> float:
        return self.current_bound


class NoiseScheduler:
    """å™ªå£°æ°´å¹³è°ƒåº¦å™¨"""

    def __init__(self, config: DPConfig):
        self.config = config
        self.initial_noise = config.noise_multiplier

    def get_noise_multiplier(self, epoch: int) -> float:
        """æ ¹æ®è®­ç»ƒè¿›åº¦è°ƒæ•´å™ªå£°æ°´å¹³"""
        if not self.config.adaptive_noise:
            return self.initial_noise

        # çº¿æ€§è¡°å‡ç­–ç•¥
        progress = epoch / self.config.num_epochs
        min_noise = self.initial_noise * 0.5

        noise = self.initial_noise - (self.initial_noise - min_noise) * progress
        return max(min_noise, noise)


class DifferentialPrivacyManager:
    """å·®åˆ†éšç§ç®¡ç†å™¨ - æ ¸å¿ƒDP-FedAvgå®ç°"""

    def __init__(self, config: DPConfig):
        self.config = config
        self.config.validate()

        self.accountant = PrivacyAccountant(
            delta=config.target_delta,
            accountant_type=config.accountant_type
        )

        # è‡ªé€‚åº”å‚æ•°
        self.adaptive_clipping = AdaptiveClipping(config.clipping_bound) if config.auto_clip else None
        self.noise_scheduler = NoiseScheduler(config) if config.adaptive_noise else None

        # ç»Ÿè®¡ä¿¡æ¯
        self.stats = {
            'total_clipped_clients': 0,
            'avg_norm_before_clip': [],
            'avg_norm_after_clip': [],
            'noise_levels': [],
            'privacy_budget_history': []
        }

        logger.info(f"DP Manager initialized with Îµ={config.target_epsilon}, Î´={config.target_delta}")

    def process_client_update(self,
                              initial_params: Dict[str, torch.Tensor],
                              updated_params: Dict[str, torch.Tensor],
                              client_id: int = None) -> Dict[str, torch.Tensor]:
        """å¤„ç†å•ä¸ªå®¢æˆ·ç«¯æ›´æ–° - è®¡ç®—å¢é‡å¹¶å‰ªè£"""

        # 1. è®¡ç®—å‚æ•°æ›´æ–°å¢é‡
        update_delta = {}
        for key in initial_params:
            if key in updated_params and initial_params[key].shape == updated_params[key].shape:
                delta = updated_params[key] - initial_params[key]
                update_delta[key] = delta

        # 2. è®¡ç®—L2èŒƒæ•°
        total_norm = self._compute_l2_norm(update_delta)
        self.stats['avg_norm_before_clip'].append(total_norm)

        # 3. åº”ç”¨å‰ªè£
        clipped_update = self._clip_update(update_delta, total_norm)

        # 4. è®°å½•ç»Ÿè®¡ä¿¡æ¯
        clipped_norm = self._compute_l2_norm(clipped_update)
        self.stats['avg_norm_after_clip'].append(clipped_norm)

        if total_norm > self.config.clipping_bound:
            self.stats['total_clipped_clients'] += 1
            if client_id is not None:
                logger.debug(f"Client {client_id}: clipped norm {total_norm:.4f} -> {clipped_norm:.4f}")

        # 5. è‡ªé€‚åº”å‰ªè£é˜ˆå€¼è°ƒæ•´
        if self.adaptive_clipping:
            self.adaptive_clipping.update(total_norm)
            self.config.clipping_bound = self.adaptive_clipping.get_threshold()

        return clipped_update

    def _compute_l2_norm(self, param_dict: Dict[str, torch.Tensor]) -> float:
        """è®¡ç®—å‚æ•°å­—å…¸çš„L2èŒƒæ•°"""
        total_norm_squared = 0.0
        for tensor in param_dict.values():
            if tensor.is_floating_point():
                total_norm_squared += tensor.norm().item() ** 2
        return math.sqrt(total_norm_squared)

    def _clip_update(self, update_delta: Dict[str, torch.Tensor],
                     total_norm: float) -> Dict[str, torch.Tensor]:
        """åº”ç”¨L2èŒƒæ•°å‰ªè£"""
        if total_norm <= self.config.clipping_bound:
            return update_delta

        # è®¡ç®—å‰ªè£å› å­
        clip_factor = self.config.clipping_bound / total_norm

        clipped_update = {}
        for key, tensor in update_delta.items():
            if tensor.is_floating_point():
                clipped_update[key] = tensor * clip_factor
            else:
                clipped_update[key] = tensor.clone()

        return clipped_update

    def aggregate_and_add_noise(self,
                                client_updates: List[Dict[str, torch.Tensor]],
                                epoch: int = None) -> Dict[str, torch.Tensor]:
        """èšåˆå®¢æˆ·ç«¯æ›´æ–°å¹¶æ·»åŠ DPå™ªå£°"""

        if not client_updates:
            raise ValueError("No client updates to aggregate")

        # 1. è”é‚¦å¹³å‡
        aggregated_update = self._federated_averaging(client_updates)

        # 2. è®¡ç®—å™ªå£°å‚æ•°
        current_noise_multiplier = self.config.noise_multiplier
        if self.noise_scheduler:
            current_noise_multiplier = self.noise_scheduler.get_noise_multiplier(epoch)

        # 3. æ·»åŠ é«˜æ–¯å™ªå£°
        noisy_update = self._add_gaussian_noise(aggregated_update, current_noise_multiplier)

        # 4. æ›´æ–°éšç§é¢„ç®—
        sampling_rate = len(client_updates) / self.config.num_clients
        current_epsilon = self.accountant.step(
            current_noise_multiplier,
            sampling_rate,
            len(client_updates)
        )

        # 5. è®°å½•ç»Ÿè®¡ä¿¡æ¯
        self.stats['noise_levels'].append(current_noise_multiplier)
        self.stats['privacy_budget_history'].append(current_epsilon)

        # 6. æ£€æŸ¥éšç§é¢„ç®—æ˜¯å¦è¶…é™
        if current_epsilon > self.config.target_epsilon:
            logger.warning(f"Privacy budget exceeded! Current Îµ={current_epsilon:.4f}, "
                           f"Target Îµ={self.config.target_epsilon:.4f}")

        logger.info(f"Aggregated {len(client_updates)} updates, Îµ={current_epsilon:.4f}, "
                    f"noise_mult={current_noise_multiplier:.4f}")

        return noisy_update

    def _federated_averaging(self,
                             client_updates: List[Dict[str, torch.Tensor]]) -> Dict[str, torch.Tensor]:
        """æ‰§è¡Œè”é‚¦å¹³å‡"""
        if not client_updates:
            return {}

        # åˆå§‹åŒ–èšåˆç»“æœ
        aggregated = {}
        first_update = client_updates[0]

        for key in first_update:
            aggregated[key] = torch.zeros_like(first_update[key])

        # ç´¯åŠ æ‰€æœ‰å®¢æˆ·ç«¯æ›´æ–°
        for update in client_updates:
            for key in aggregated:
                if key in update:
                    aggregated[key] += update[key]

        # å–å¹³å‡
        num_clients = len(client_updates)
        for key in aggregated:
            aggregated[key] /= num_clients

        return aggregated

    def _add_gaussian_noise(self,
                            aggregated_update: Dict[str, torch.Tensor],
                            noise_multiplier: float) -> Dict[str, torch.Tensor]:
        """æ·»åŠ æ ¡å‡†çš„é«˜æ–¯å™ªå£°"""
        sigma = noise_multiplier * self.config.clipping_bound
        noisy_update = {}

        for key, tensor in aggregated_update.items():
            if tensor.is_floating_point():
                # ç”Ÿæˆä¸å¼ é‡å½¢çŠ¶ç›¸åŒçš„é«˜æ–¯å™ªå£°
                noise = torch.normal(
                    mean=0.0,
                    std=sigma,
                    size=tensor.shape,
                    device=tensor.device,
                    dtype=tensor.dtype
                )
                noisy_update[key] = tensor + noise
            else:
                # éæµ®ç‚¹å‚æ•°ä¸åŠ å™ªå£°
                noisy_update[key] = tensor.clone()

        return noisy_update

    def get_privacy_budget(self) -> Tuple[float, float]:
        """è·å–å½“å‰éšç§é¢„ç®—"""
        return self.accountant.get_privacy_spent()

    def get_remaining_budget(self) -> float:
        """è·å–å‰©ä½™éšç§é¢„ç®—"""
        current_eps, _ = self.get_privacy_budget()
        return max(0, self.config.target_epsilon - current_eps)

    def can_continue_training(self) -> bool:
        """æ£€æŸ¥æ˜¯å¦å¯ä»¥ç»§ç»­è®­ç»ƒ"""
        current_eps, _ = self.get_privacy_budget()
        return current_eps < self.config.target_epsilon

    def get_statistics(self) -> Dict[str, Any]:
        """è·å–è¯¦ç»†ç»Ÿè®¡ä¿¡æ¯"""
        stats = self.stats.copy()
        current_eps, delta = self.get_privacy_budget()

        stats.update({
            'current_epsilon': current_eps,
            'delta': delta,
            'target_epsilon': self.config.target_epsilon,
            'remaining_budget': self.get_remaining_budget(),
            'total_steps': self.accountant.steps,
            'clipping_rate': self.stats['total_clipped_clients'] / max(1, len(self.stats['avg_norm_before_clip'])),
            'avg_norm_reduction': np.mean(self.stats['avg_norm_after_clip']) / max(1e-8, np.mean(
                self.stats['avg_norm_before_clip']))
        })

        return stats


class FederatedDPTraining:
    """
    è”é‚¦å·®åˆ†éšç§è®­ç»ƒåè°ƒå™¨ - å®Œæ•´å®ç°
    é›†æˆFed-Prunerçš„è’¸é¦è®­ç»ƒã€æ¨¡å‹å‰ªæå’ŒDPä¿æŠ¤
    """

    def __init__(self, dp_config: DPConfig):
        self.dp_manager = DifferentialPrivacyManager(dp_config)
        self.config = dp_config

        # è®­ç»ƒçŠ¶æ€è¿½è¸ª
        self.current_round = 0
        self.training_history = []

        logger.info("ğŸ”’ FederatedDPTraining initialized with DP protection")

    def client_training_step(self,
                             client_trainer,  # DistillTrainerå®ä¾‹
                             server_weights: Dict[str, torch.Tensor],
                             client_train_data,  # å®¢æˆ·ç«¯è®­ç»ƒæ•°æ®
                             client_id: int,
                             training_args,  # TrainingArguments
                             teacher_model: Optional[nn.Module] = None) -> Dict[str, torch.Tensor]:
        """
        å®Œæ•´çš„å®¢æˆ·ç«¯DPè®­ç»ƒæ­¥éª¤

        Args:
            client_trainer: Fed-Prunerçš„DistillTrainerå®ä¾‹
            server_weights: æœåŠ¡å™¨ä¸‹å‘çš„æ¨¡å‹æƒé‡
            client_train_data: å®¢æˆ·ç«¯è®­ç»ƒæ•°æ®
            client_id: å®¢æˆ·ç«¯ID
            training_args: è®­ç»ƒå‚æ•°
            teacher_model: æ•™å¸ˆæ¨¡å‹ï¼ˆç”¨äºçŸ¥è¯†è’¸é¦ï¼‰

        Returns:
            å‰ªè£åçš„æ¨¡å‹æ›´æ–°å¢é‡
        """

        logger.info(f"ğŸ”’ Starting DP client training for client {client_id}")
        start_time = time.time()

        try:
            # ========== 1. ä¿å­˜åˆå§‹æƒé‡ ==========
            initial_weights = deepcopy(server_weights)

            # ========== 2. å‡†å¤‡å®¢æˆ·ç«¯æ¨¡å‹ ==========
            client_model = client_trainer.model

            # ç¡®ä¿æ¨¡å‹åœ¨æ­£ç¡®çš„è®¾å¤‡ä¸Š
            device = next(client_model.parameters()).device

            # åŠ è½½æœåŠ¡å™¨æƒé‡åˆ°å®¢æˆ·ç«¯æ¨¡å‹
            try:
                client_model.load_state_dict(server_weights, strict=False)
                logger.debug(f"Client {client_id}: Loaded server weights successfully")
            except Exception as e:
                logger.error(f"Client {client_id}: Failed to load server weights: {e}")
                # å¦‚æœåŠ è½½å¤±è´¥ï¼Œä½¿ç”¨å…¼å®¹æ€§åŠ è½½
                self._safe_load_state_dict(client_model, server_weights)

            # ========== 3. è®¾ç½®è®­ç»ƒç¯å¢ƒ ==========
            client_model.train()

            # åˆ›å»ºæ•°æ®åŠ è½½å™¨ï¼ˆå¦‚æœéœ€è¦ï¼‰
            if hasattr(client_train_data, '__iter__'):
                train_dataloader = client_train_data
            else:
                # å‡è®¾client_train_dataæ˜¯æ•°æ®é›†ï¼Œéœ€è¦åˆ›å»ºDataLoader
                from torch.utils.data import DataLoader
                train_dataloader = DataLoader(
                    client_train_data,
                    batch_size=training_args.per_device_train_batch_size,
                    shuffle=True
                )

            # ========== 4. æœ¬åœ°è®­ç»ƒå¾ªç¯ ==========
            num_local_epochs = getattr(training_args, 'local_epochs', 1)
            total_loss = 0.0
            num_batches = 0

            for local_epoch in range(num_local_epochs):
                epoch_loss = 0.0
                epoch_batches = 0

                for batch_idx, batch in enumerate(train_dataloader):
                    try:
                        # å°†æ•°æ®ç§»åŠ¨åˆ°æ­£ç¡®çš„è®¾å¤‡
                        batch = self._move_batch_to_device(batch, device)

                        # ========== 4.1 å‰å‘ä¼ æ’­å’ŒæŸå¤±è®¡ç®— ==========
                        if hasattr(client_trainer, 'compute_loss'):
                            # ä½¿ç”¨Fed-Prunerçš„æŸå¤±è®¡ç®—ï¼ˆåŒ…å«è’¸é¦æŸå¤±ï¼‰
                            loss = client_trainer.compute_loss(client_model, batch)
                        else:
                            # å›é€€åˆ°åŸºæœ¬æŸå¤±è®¡ç®—
                            loss = self._compute_basic_loss(client_model, batch)

                        # ========== 4.2 åå‘ä¼ æ’­ ==========
                        client_trainer.optimizer.zero_grad()
                        loss.backward()

                        # ========== 4.3 æ¢¯åº¦å¤„ç†ï¼ˆå¯é€‰çš„æ¢¯åº¦å‰ªè£ï¼‰ ==========
                        if hasattr(training_args, 'max_grad_norm') and training_args.max_grad_norm > 0:
                            torch.nn.utils.clip_grad_norm_(
                                client_model.parameters(),
                                training_args.max_grad_norm
                            )

                        # ========== 4.4 å‚æ•°æ›´æ–° ==========
                        client_trainer.optimizer.step()

                        # å¦‚æœæœ‰å­¦ä¹ ç‡è°ƒåº¦å™¨
                        if hasattr(client_trainer, 'lr_scheduler') and client_trainer.lr_scheduler:
                            client_trainer.lr_scheduler.step()

                        # ========== 4.5 è®°å½•è®­ç»ƒç»Ÿè®¡ ==========
                        epoch_loss += loss.item()
                        epoch_batches += 1

                        # å¯é€‰ï¼šé™åˆ¶è®­ç»ƒæ­¥æ•°ï¼ˆç”¨äºå¿«é€Ÿå®éªŒï¼‰
                        if hasattr(training_args, 'max_steps_per_client'):
                            if batch_idx >= training_args.max_steps_per_client:
                                break

                    except Exception as e:
                        logger.error(f"Client {client_id}: Training step failed: {e}")
                        continue

                avg_epoch_loss = epoch_loss / max(1, epoch_batches)
                logger.debug(f"Client {client_id}: Local epoch {local_epoch + 1}, avg loss: {avg_epoch_loss:.4f}")

                total_loss += epoch_loss
                num_batches += epoch_batches

            # ========== 5. è·å–è®­ç»ƒåçš„æƒé‡ ==========
            updated_weights = self._create_safe_state_dict(client_model)

            if updated_weights is None:
                logger.error(f"Client {client_id}: Failed to extract updated weights")
                return self._create_zero_update(initial_weights)

            # ========== 6. DPå¤„ç†ï¼šè®¡ç®—å¹¶å‰ªè£æ›´æ–°å¢é‡ ==========
            clipped_update = self.dp_manager.process_client_update(
                initial_weights, updated_weights, client_id
            )

            # ========== 7. è®°å½•è®­ç»ƒç»Ÿè®¡ ==========
            training_time = time.time() - start_time
            avg_loss = total_loss / max(1, num_batches)

            self._record_client_training_stats(
                client_id, training_time, avg_loss, num_batches
            )

            logger.info(f"ğŸ”’ Client {client_id} DP training completed: "
                        f"time={training_time:.2f}s, avg_loss={avg_loss:.4f}")

            return clipped_update

        except Exception as e:
            logger.error(f"Client {client_id} DP training failed: {e}")
            # è¿”å›é›¶æ›´æ–°ï¼Œé¿å…è®­ç»ƒä¸­æ–­
            return self._create_zero_update(initial_weights)

    def server_aggregation_step(self,
                                client_updates: List[Dict[str, torch.Tensor]],
                                epoch: int) -> Dict[str, torch.Tensor]:
        """
        æœåŠ¡å™¨DPèšåˆæ­¥éª¤ï¼ˆæ‰©å±•ç‰ˆæœ¬ï¼‰

        Args:
            client_updates: å®¢æˆ·ç«¯æ›´æ–°åˆ—è¡¨
            epoch: å½“å‰è½®æ¬¡

        Returns:
            åŠ å™ªåçš„èšåˆæ›´æ–°
        """

        if not client_updates:
            logger.error("No client updates received for aggregation")
            return {}

        logger.info(f"ğŸ”’ Starting DP aggregation for round {epoch + 1}")
        start_time = time.time()

        # è¿‡æ»¤æ— æ•ˆæ›´æ–°
        valid_updates = [update for update in client_updates if update and len(update) > 0]

        if not valid_updates:
            logger.error("No valid client updates for aggregation")
            return {}

        logger.info(f"Aggregating {len(valid_updates)}/{len(client_updates)} valid updates")

        # æ‰§è¡ŒDPèšåˆ
        try:
            noisy_update = self.dp_manager.aggregate_and_add_noise(valid_updates, epoch)

            aggregation_time = time.time() - start_time

            # è®°å½•èšåˆç»Ÿè®¡
            current_eps, delta = self.dp_manager.get_privacy_budget()
            remaining_budget = self.dp_manager.get_remaining_budget()

            logger.info(f"ğŸ”’ DP aggregation completed: time={aggregation_time:.2f}s, "
                        f"Îµ={current_eps:.4f}, remaining={remaining_budget:.4f}")

            # æ›´æ–°è®­ç»ƒå†å²
            self.training_history.append({
                'round': epoch + 1,
                'num_clients': len(valid_updates),
                'privacy_budget': current_eps,
                'aggregation_time': aggregation_time
            })

            return noisy_update

        except Exception as e:
            logger.error(f"DP aggregation failed: {e}")
            return {}

    def apply_update_to_server(self,
                               server_model: nn.Module,
                               aggregated_update: Dict[str, torch.Tensor],
                               learning_rate: float = 1.0):
        """
        å°†èšåˆæ›´æ–°åº”ç”¨åˆ°æœåŠ¡å™¨æ¨¡å‹ï¼ˆæ‰©å±•ç‰ˆæœ¬ï¼‰

        Args:
            server_model: æœåŠ¡å™¨æ¨¡å‹
            aggregated_update: èšåˆæ›´æ–°
            learning_rate: æœåŠ¡å™¨ç«¯å­¦ä¹ ç‡
        """

        if not aggregated_update:
            logger.warning("No aggregated update to apply")
            return

        try:
            with torch.no_grad():
                server_state = server_model.state_dict()
                updated_params = 0

                for key, update in aggregated_update.items():
                    if key in server_state and server_state[key].shape == update.shape:
                        # åº”ç”¨æ›´æ–°ï¼šæ–°æƒé‡ = æ—§æƒé‡ + å­¦ä¹ ç‡ * æ›´æ–°
                        server_state[key] += learning_rate * update.to(server_state[key].device)
                        updated_params += 1
                    else:
                        logger.warning(f"Skipping parameter {key}: shape mismatch or not found")

                server_model.load_state_dict(server_state)
                logger.info(f"Applied updates to {updated_params} parameters")

        except Exception as e:
            logger.error(f"Failed to apply server update: {e}")

    # ========== è¾…åŠ©æ–¹æ³• ==========

    def _safe_load_state_dict(self, model: nn.Module, state_dict: Dict[str, torch.Tensor]):
        """å®‰å…¨åœ°åŠ è½½çŠ¶æ€å­—å…¸ï¼Œå¤„ç†é”®åä¸åŒ¹é…çš„æƒ…å†µ"""
        model_state = model.state_dict()

        # æ‰¾åˆ°åŒ¹é…çš„é”®
        matched_keys = []
        for key in state_dict:
            if key in model_state and model_state[key].shape == state_dict[key].shape:
                matched_keys.append(key)

        # åªåŠ è½½åŒ¹é…çš„å‚æ•°
        filtered_state_dict = {k: state_dict[k] for k in matched_keys}
        model.load_state_dict(filtered_state_dict, strict=False)

        logger.info(f"Loaded {len(matched_keys)}/{len(state_dict)} parameters successfully")

    def _move_batch_to_device(self, batch, device):
        """å°†æ‰¹æ¬¡æ•°æ®ç§»åŠ¨åˆ°æŒ‡å®šè®¾å¤‡"""
        if isinstance(batch, dict):
            return {k: v.to(device) if hasattr(v, 'to') else v for k, v in batch.items()}
        elif isinstance(batch, (list, tuple)):
            return [item.to(device) if hasattr(item, 'to') else item for item in batch]
        else:
            return batch.to(device) if hasattr(batch, 'to') else batch

    def _compute_basic_loss(self, model: nn.Module, batch) -> torch.Tensor:
        """è®¡ç®—åŸºæœ¬æŸå¤±ï¼ˆå½“æ²¡æœ‰è‡ªå®šä¹‰compute_lossæ—¶ä½¿ç”¨ï¼‰"""
        if isinstance(batch, dict):
            # å‡è®¾æ˜¯transformersæ ¼å¼
            outputs = model(**batch)
            return outputs.loss if hasattr(outputs, 'loss') else outputs['loss']
        else:
            # å‡è®¾æ˜¯(inputs, labels)æ ¼å¼
            inputs, labels = batch
            outputs = model(inputs)
            loss_fn = nn.CrossEntropyLoss()
            return loss_fn(outputs, labels)

    def _create_safe_state_dict(self, model: nn.Module) -> Optional[Dict[str, torch.Tensor]]:
        """å®‰å…¨åœ°åˆ›å»ºçŠ¶æ€å­—å…¸"""
        try:
            state_dict = {}
            for name, param in model.named_parameters():
                if param.requires_grad:
                    state_dict[name] = param.data.clone().detach()

            # ä¹ŸåŒ…å«bufferå‚æ•°
            for name, buffer in model.named_buffers():
                state_dict[name] = buffer.clone().detach()

            return state_dict
        except Exception as e:
            logger.error(f"Failed to create state dict: {e}")
            return None

    def _create_zero_update(self, reference_weights: Dict[str, torch.Tensor]) -> Dict[str, torch.Tensor]:
        """åˆ›å»ºé›¶æ›´æ–°ï¼ˆå½“è®­ç»ƒå¤±è´¥æ—¶ä½¿ç”¨ï¼‰"""
        zero_update = {}
        for key, tensor in reference_weights.items():
            zero_update[key] = torch.zeros_like(tensor)
        return zero_update

    def _record_client_training_stats(self, client_id: int, training_time: float,
                                      avg_loss: float, num_batches: int):
        """è®°å½•å®¢æˆ·ç«¯è®­ç»ƒç»Ÿè®¡"""
        # è¿™é‡Œå¯ä»¥æ‰©å±•ä¸ºæ›´è¯¦ç»†çš„ç»Ÿè®¡è®°å½•
        logger.debug(f"Client {client_id} stats: time={training_time:.2f}s, "
                     f"loss={avg_loss:.4f}, batches={num_batches}")

    # ========== çŠ¶æ€æŸ¥è¯¢æ–¹æ³• ==========

    def get_privacy_status(self) -> Dict[str, Any]:
        """è·å–å½“å‰éšç§çŠ¶æ€"""
        current_eps, delta = self.dp_manager.get_privacy_budget()
        remaining = self.dp_manager.get_remaining_budget()

        return {
            'current_epsilon': current_eps,
            'target_epsilon': self.config.target_epsilon,
            'delta': delta,
            'remaining_budget': remaining,
            'budget_utilization': current_eps / self.config.target_epsilon,
            'can_continue_training': self.dp_manager.can_continue_training(),
            'total_rounds': len(self.training_history)
        }

    def get_training_summary(self) -> Dict[str, Any]:
        """è·å–è®­ç»ƒæ‘˜è¦"""
        if not self.training_history:
            return {'error': 'No training history available'}

        total_clients = sum(round_info['num_clients'] for round_info in self.training_history)
        avg_clients_per_round = total_clients / len(self.training_history)
        total_time = sum(round_info['aggregation_time'] for round_info in self.training_history)

        return {
            'total_rounds': len(self.training_history),
            'total_clients_trained': total_clients,
            'avg_clients_per_round': avg_clients_per_round,
            'total_aggregation_time': total_time,
            'avg_time_per_round': total_time / len(self.training_history),
            'privacy_status': self.get_privacy_status()
        }

    def reset_training_state(self):
        """é‡ç½®è®­ç»ƒçŠ¶æ€ï¼ˆç”¨äºæ–°å®éªŒï¼‰"""
        self.current_round = 0
        self.training_history.clear()
        self.dp_manager.accountant.reset()
        logger.info("Training state reset")


# ========== ä¾¿æ·é›†æˆå‡½æ•° ==========

def create_dp_federated_trainer(training_args,
                                dp_config: Optional[DPConfig] = None) -> FederatedDPTraining:
    """
    æ ¹æ®training_argsåˆ›å»ºDPè”é‚¦è®­ç»ƒå™¨

    Args:
        training_args: Fed-Prunerçš„TrainingArguments
        dp_config: å¯é€‰çš„DPé…ç½®ï¼ˆå¦‚æœNoneåˆ™ä»training_argsåˆ›å»ºï¼‰

    Returns:
        é…ç½®å¥½çš„FederatedDPTrainingå®ä¾‹
    """

    if dp_config is None:
        # ä»training_argsåˆ›å»ºDPé…ç½®
        dp_config = DPConfig(
            target_epsilon=getattr(training_args, 'dp_target_epsilon', 10.0),
            target_delta=getattr(training_args, 'dp_target_delta', 1e-5),
            noise_multiplier=getattr(training_args, 'dp_noise_multiplier', 1.0),
            clipping_bound=getattr(training_args, 'dp_clipping_bound', 1.0),
            num_epochs=getattr(training_args, 'num_train_epochs', 10),
            num_clients=getattr(training_args, 'num_clients', 10),
            accountant_type=getattr(training_args, 'dp_accountant_type', 'rdp'),
            auto_clip=getattr(training_args, 'dp_auto_clip', True),
            adaptive_noise=getattr(training_args, 'dp_adaptive_noise', True)
        )

    return FederatedDPTraining(dp_config)


def integrate_dp_with_client_trainer(client_trainer, dp_trainer: FederatedDPTraining):
    """
    å°†DPè®­ç»ƒå™¨ä¸ç°æœ‰çš„å®¢æˆ·ç«¯è®­ç»ƒå™¨é›†æˆ

    Args:
        client_trainer: Fed-Prunerçš„DistillTraineræˆ–å…¶ä»–è®­ç»ƒå™¨
        dp_trainer: DPè”é‚¦è®­ç»ƒå™¨

    Returns:
        é›†æˆåçš„è®­ç»ƒå™¨
    """

    # ä¿å­˜åŸå§‹çš„è®­ç»ƒæ–¹æ³•
    original_train_step = getattr(client_trainer, 'training_step', None)

    def dp_enhanced_training_step(model, inputs, **kwargs):
        """DPå¢å¼ºçš„è®­ç»ƒæ­¥éª¤"""
        # æ‰§è¡ŒåŸå§‹è®­ç»ƒæ­¥éª¤
        if original_train_step:
            return original_train_step(model, inputs, **kwargs)
        else:
            # é»˜è®¤è®­ç»ƒæ­¥éª¤
            return client_trainer.compute_loss(model, inputs)

    # æ›¿æ¢è®­ç»ƒæ­¥éª¤
    client_trainer.training_step = dp_enhanced_training_step
    client_trainer.dp_trainer = dp_trainer

    logger.info("Client trainer integrated with DP protection")
    return client_trainer

# å¯¼å‡ºä¸»è¦ç±»
__all__ = [
    'DPConfig',
    'PrivacyAccountant',
    'DifferentialPrivacyManager',
    'FederatedDPTraining',
    'AdaptiveClipping',
    'NoiseScheduler'
]
