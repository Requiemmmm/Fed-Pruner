# pipeline/dp_utils.py
# Fed-Prunerå·®åˆ†éšç§å·¥å…·å‡½æ•°å’Œè¾…åŠ©ç»„ä»¶
# åŒ…å«éªŒè¯ã€åˆ†æã€å¯è§†åŒ–å’Œè°ƒè¯•å·¥å…·

import torch
import torch.nn as nn
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import logging
import json
import os
from typing import Dict, List, Tuple, Optional, Union, Any
from collections import defaultdict, OrderedDict
from copy import deepcopy
import warnings
from dataclasses import asdict

from .dp_core import DPConfig, DifferentialPrivacyManager, PrivacyAccountant

logger = logging.getLogger(__name__)


# ================================ éªŒè¯å·¥å…· ================================

def validate_dp_implementation(dp_manager: DifferentialPrivacyManager,
                               num_test_rounds: int = 5) -> Dict[str, bool]:
    """
    éªŒè¯DPå®ç°çš„æ­£ç¡®æ€§

    Args:
        dp_manager: DPç®¡ç†å™¨å®ä¾‹
        num_test_rounds: æµ‹è¯•è½®æ•°

    Returns:
        éªŒè¯ç»“æœå­—å…¸
    """
    results = {}

    logger.info("ğŸ§ª Validating DP implementation...")

    # æµ‹è¯•1: å‰ªè£æœºåˆ¶
    logger.info("Testing clipping mechanism...")
    try:
        initial_params = {"test_param": torch.randn(10, 10) * 0.1}
        large_update_params = {"test_param": torch.randn(10, 10) * 10.0}

        clipped_update = dp_manager.process_client_update(
            initial_params, large_update_params, 0
        )

        norm = np.sqrt(sum(tensor.norm().item() ** 2 for tensor in clipped_update.values()))
        results['clipping_works'] = norm <= dp_manager.config.clipping_bound + 1e-6

        if results['clipping_works']:
            logger.info(f"âœ… Clipping works: norm={norm:.4f} â‰¤ bound={dp_manager.config.clipping_bound}")
        else:
            logger.error(f"âŒ Clipping failed: norm={norm:.4f} > bound={dp_manager.config.clipping_bound}")

    except Exception as e:
        logger.error(f"âŒ Clipping test failed: {e}")
        results['clipping_works'] = False

    # æµ‹è¯•2: å™ªå£°æ·»åŠ 
    logger.info("Testing noise addition...")
    try:
        client_updates = []
        for i in range(3):
            update = {"param": torch.randn(5, 5) * 0.1}
            client_updates.append(update)

        noisy_update = dp_manager.aggregate_and_add_noise(client_updates, 0)

        results['noise_addition_works'] = (
                isinstance(noisy_update, dict) and
                all(isinstance(v, torch.Tensor) for v in noisy_update.values())
        )

        if results['noise_addition_works']:
            logger.info("âœ… Noise addition works correctly")
        else:
            logger.error("âŒ Noise addition failed")

    except Exception as e:
        logger.error(f"âŒ Noise addition test failed: {e}")
        results['noise_addition_works'] = False

    # æµ‹è¯•3: éšç§é¢„ç®—è¿½è¸ª
    logger.info("Testing privacy budget tracking...")
    try:
        initial_eps, _ = dp_manager.get_privacy_budget()

        # æ¨¡æ‹Ÿå‡ è½®è®­ç»ƒ
        eps_values = [initial_eps]
        for round_num in range(num_test_rounds):
            eps = dp_manager.accountant.step(1.0, 1.0, dp_manager.config.num_clients)
            eps_values.append(eps)

        # æ£€æŸ¥epsilonæ˜¯å¦å•è°ƒé€’å¢
        is_monotonic = all(eps_values[i] <= eps_values[i + 1] for i in range(len(eps_values) - 1))
        results['budget_tracking_works'] = is_monotonic and eps_values[-1] > initial_eps

        if results['budget_tracking_works']:
            logger.info(f"âœ… Budget tracking works: Îµ increased from {initial_eps:.4f} to {eps_values[-1]:.4f}")
        else:
            logger.error("âŒ Budget tracking failed")

    except Exception as e:
        logger.error(f"âŒ Budget tracking test failed: {e}")
        results['budget_tracking_works'] = False

    # ç»¼åˆç»“æœ
    all_passed = all(results.values())
    results['overall_passed'] = all_passed

    if all_passed:
        logger.info("ğŸ‰ All DP validation tests passed!")
    else:
        failed_tests = [test for test, passed in results.items() if not passed and test != 'overall_passed']
        logger.warning(f"âš ï¸  Failed tests: {failed_tests}")

    return results


def check_privacy_budget_consumption(dp_manager: DifferentialPrivacyManager) -> Dict[str, float]:
    """
    æ£€æŸ¥éšç§é¢„ç®—æ¶ˆè€—æƒ…å†µ

    Returns:
        éšç§é¢„ç®—ç»Ÿè®¡ä¿¡æ¯
    """
    current_eps, delta = dp_manager.get_privacy_budget()
    remaining = dp_manager.get_remaining_budget()
    utilization = current_eps / dp_manager.config.target_epsilon

    budget_info = {
        'current_epsilon': current_eps,
        'target_epsilon': dp_manager.config.target_epsilon,
        'delta': delta,
        'remaining_budget': remaining,
        'utilization_percentage': utilization * 100,
        'can_continue': dp_manager.can_continue_training()
    }

    logger.info(f"Privacy budget status: Îµ={current_eps:.4f}/{dp_manager.config.target_epsilon:.4f} "
                f"({utilization:.1%} used)")

    return budget_info


# ================================ åˆ†æå·¥å…· ================================

def analyze_clipping_statistics(dp_manager: DifferentialPrivacyManager) -> Dict[str, Any]:
    """
    åˆ†æå‰ªè£ç»Ÿè®¡ä¿¡æ¯

    Returns:
        å‰ªè£åˆ†æç»“æœ
    """
    stats = dp_manager.get_statistics()

    if not stats['avg_norm_before_clip']:
        return {'error': 'No clipping data available'}

    norms_before = np.array(stats['avg_norm_before_clip'])
    norms_after = np.array(stats['avg_norm_after_clip'])

    analysis = {
        'total_updates': len(norms_before),
        'clipped_updates': stats['total_clipped_clients'],
        'clipping_rate': stats['clipping_rate'],
        'avg_norm_before': np.mean(norms_before),
        'avg_norm_after': np.mean(norms_after),
        'max_norm_before': np.max(norms_before),
        'min_norm_before': np.min(norms_before),
        'norm_reduction_ratio': stats['avg_norm_reduction'],
        'current_clipping_bound': dp_manager.config.clipping_bound
    }

    # è®¡ç®—åˆ†ä½æ•°
    analysis.update({
        'norm_percentiles': {
            'p25': np.percentile(norms_before, 25),
            'p50': np.percentile(norms_before, 50),
            'p75': np.percentile(norms_before, 75),
            'p90': np.percentile(norms_before, 90),
            'p95': np.percentile(norms_before, 95)
        }
    })

    return analysis


def compute_privacy_utility_curve(base_accuracy: float,
                                  epsilon_values: List[float],
                                  accuracy_drops: List[float]) -> Dict[str, List[float]]:
    """
    è®¡ç®—éšç§-æ•ˆç”¨æƒè¡¡æ›²çº¿

    Args:
        base_accuracy: åŸºçº¿å‡†ç¡®ç‡ï¼ˆæ— DPï¼‰
        epsilon_values: epsilonå€¼åˆ—è¡¨
        accuracy_drops: å¯¹åº”çš„å‡†ç¡®ç‡ä¸‹é™åˆ—è¡¨

    Returns:
        éšç§-æ•ˆç”¨æ›²çº¿æ•°æ®
    """
    if len(epsilon_values) != len(accuracy_drops):
        raise ValueError("epsilon_values and accuracy_drops must have same length")

    dp_accuracies = [base_accuracy - drop for drop in accuracy_drops]
    privacy_levels = [1 / eps if eps > 0 else float('inf') for eps in epsilon_values]

    return {
        'epsilon_values': epsilon_values,
        'dp_accuracies': dp_accuracies,
        'accuracy_drops': accuracy_drops,
        'privacy_levels': privacy_levels,
        'base_accuracy': base_accuracy
    }


# ================================ æ•°æ®å¤„ç†å·¥å…· ================================

def create_non_iid_data_split(dataset,
                              num_clients: int,
                              alpha: float = 0.5,
                              min_samples_per_client: int = 10) -> List[List[int]]:
    """
    ä½¿ç”¨Dirichletåˆ†å¸ƒåˆ›å»ºNon-IIDæ•°æ®åˆ’åˆ†

    Args:
        dataset: æ•°æ®é›†
        num_clients: å®¢æˆ·ç«¯æ•°é‡
        alpha: Dirichletåˆ†å¸ƒçš„æµ“åº¦å‚æ•°
        min_samples_per_client: æ¯ä¸ªå®¢æˆ·ç«¯æœ€å°æ ·æœ¬æ•°

    Returns:
        æ¯ä¸ªå®¢æˆ·ç«¯çš„æ•°æ®ç´¢å¼•åˆ—è¡¨
    """
    # è·å–æ ‡ç­¾
    if hasattr(dataset, 'targets'):
        labels = dataset.targets
    elif hasattr(dataset, 'labels'):
        labels = dataset.labels
    else:
        # å‡è®¾æ•°æ®é›†æœ‰labelå­—æ®µ
        labels = [item['label'] if isinstance(item, dict) else item[1] for item in dataset]

    if isinstance(labels, torch.Tensor):
        labels = labels.numpy()

    # æŒ‰æ ‡ç­¾åˆ†ç»„æ•°æ®ç´¢å¼•
    unique_labels = np.unique(labels)
    label_to_indices = {label: [] for label in unique_labels}

    for idx, label in enumerate(labels):
        label_to_indices[label].append(idx)

    client_indices = [[] for _ in range(num_clients)]

    # å¯¹æ¯ä¸ªæ ‡ç­¾ä½¿ç”¨Dirichletåˆ†å¸ƒåˆ†é…
    for label in unique_labels:
        indices = label_to_indices[label]
        np.random.shuffle(indices)

        # ç”ŸæˆDirichletåˆ†å¸ƒçš„æƒé‡
        proportions = np.random.dirichlet(np.repeat(alpha, num_clients))

        # ç¡®ä¿æ¯ä¸ªå®¢æˆ·ç«¯è‡³å°‘æœ‰ä¸€äº›æ ·æœ¬
        proportions = np.maximum(proportions, 1e-6)
        proportions = proportions / proportions.sum()

        # æ ¹æ®æƒé‡åˆ†é…æ ·æœ¬
        start_idx = 0
        for client_id, prop in enumerate(proportions):
            end_idx = start_idx + int(prop * len(indices))
            if client_id == num_clients - 1:  # æœ€åä¸€ä¸ªå®¢æˆ·ç«¯è·å¾—å‰©ä½™æ‰€æœ‰æ ·æœ¬
                end_idx = len(indices)

            client_indices[client_id].extend(indices[start_idx:end_idx])
            start_idx = end_idx

    # ç¡®ä¿æ¯ä¸ªå®¢æˆ·ç«¯æœ‰æœ€å°æ ·æœ¬æ•°
    for client_id in range(num_clients):
        if len(client_indices[client_id]) < min_samples_per_client:
            logger.warning(f"Client {client_id} has only {len(client_indices[client_id])} samples, "
                           f"less than minimum {min_samples_per_client}")

    # æ‰“ä¹±æ¯ä¸ªå®¢æˆ·ç«¯çš„æ•°æ®
    for client_id in range(num_clients):
        np.random.shuffle(client_indices[client_id])

    # ç»Ÿè®¡ä¿¡æ¯
    total_samples = sum(len(indices) for indices in client_indices)
    logger.info(f"Non-IID data split created:")
    logger.info(f"  - {num_clients} clients")
    logger.info(f"  - {total_samples} total samples")
    logger.info(f"  - Alpha: {alpha}")
    logger.info(f"  - Samples per client: {[len(indices) for indices in client_indices]}")

    return client_indices


def compute_data_heterogeneity_metrics(client_data_indices: List[List[int]],
                                       labels: Union[List, np.ndarray, torch.Tensor]) -> Dict[str, float]:
    """
    è®¡ç®—æ•°æ®å¼‚æ„æ€§æŒ‡æ ‡

    Args:
        client_data_indices: æ¯ä¸ªå®¢æˆ·ç«¯çš„æ•°æ®ç´¢å¼•
        labels: æ•°æ®æ ‡ç­¾

    Returns:
        å¼‚æ„æ€§æŒ‡æ ‡
    """
    if isinstance(labels, torch.Tensor):
        labels = labels.numpy()
    elif isinstance(labels, list):
        labels = np.array(labels)

    num_clients = len(client_data_indices)
    unique_labels = np.unique(labels)
    num_classes = len(unique_labels)

    # è®¡ç®—æ¯ä¸ªå®¢æˆ·ç«¯çš„æ ‡ç­¾åˆ†å¸ƒ
    client_distributions = []
    for client_indices in client_data_indices:
        client_labels = labels[client_indices]
        label_counts = np.bincount(client_labels, minlength=num_classes)
        distribution = label_counts / len(client_indices) if len(client_indices) > 0 else np.zeros(num_classes)
        client_distributions.append(distribution)

    client_distributions = np.array(client_distributions)

    # è®¡ç®—å…¨å±€åˆ†å¸ƒ
    global_distribution = np.bincount(labels, minlength=num_classes) / len(labels)

    # KLæ•£åº¦ï¼ˆè¡¡é‡ä¸å…¨å±€åˆ†å¸ƒçš„å·®å¼‚ï¼‰
    kl_divergences = []
    for i in range(num_clients):
        # é¿å…é™¤é›¶ï¼Œæ·»åŠ å°çš„å¹³æ»‘é¡¹
        client_dist = client_distributions[i] + 1e-10
        global_dist = global_distribution + 1e-10

        kl_div = np.sum(client_dist * np.log(client_dist / global_dist))
        kl_divergences.append(kl_div)

    # Earth Mover's Distance (Wassersteinè·ç¦»)
    try:
        from scipy.stats import wasserstein_distance
        emd_distances = []
        for i in range(num_clients):
            emd = wasserstein_distance(
                range(num_classes), range(num_classes),
                client_distributions[i], global_distribution
            )
            emd_distances.append(emd)
        avg_emd = np.mean(emd_distances)
    except ImportError:
        logger.warning("scipy not available, skipping EMD calculation")
        avg_emd = None

    # Jensen-Shannonæ•£åº¦
    def js_divergence(p, q):
        p = p + 1e-10
        q = q + 1e-10
        m = 0.5 * (p + q)
        return 0.5 * np.sum(p * np.log(p / m)) + 0.5 * np.sum(q * np.log(q / m))

    js_divergences = []
    for i in range(num_clients):
        js_div = js_divergence(client_distributions[i], global_distribution)
        js_divergences.append(js_div)

    metrics = {
        'avg_kl_divergence': np.mean(kl_divergences),
        'max_kl_divergence': np.max(kl_divergences),
        'std_kl_divergence': np.std(kl_divergences),
        'avg_js_divergence': np.mean(js_divergences),
        'max_js_divergence': np.max(js_divergences),
        'num_clients': num_clients,
        'num_classes': num_classes,
        'samples_per_client': [len(indices) for indices in client_data_indices]
    }

    if avg_emd is not None:
        metrics['avg_emd'] = avg_emd

    return metrics


# ================================ å¯è§†åŒ–å·¥å…· ================================

def plot_privacy_budget_over_time(privacy_history: List[float],
                                  target_epsilon: float,
                                  save_path: Optional[str] = None):
    """
    ç»˜åˆ¶éšç§é¢„ç®—éšæ—¶é—´å˜åŒ–çš„å›¾è¡¨

    Args:
        privacy_history: éšç§é¢„ç®—å†å²
        target_epsilon: ç›®æ ‡epsilonå€¼
        save_path: ä¿å­˜è·¯å¾„ï¼ˆå¯é€‰ï¼‰
    """
    plt.figure(figsize=(10, 6))

    epochs = list(range(len(privacy_history)))
    plt.plot(epochs, privacy_history, 'b-', linewidth=2, label='Privacy Budget (Îµ)')
    plt.axhline(y=target_epsilon, color='r', linestyle='--',
                label=f'Target Îµ = {target_epsilon}')

    plt.xlabel('Training Rounds')
    plt.ylabel('Privacy Budget (Îµ)')
    plt.title('Privacy Budget Consumption Over Time')
    plt.legend()
    plt.grid(True, alpha=0.3)

    # æ ‡æ³¨é‡è¦ç‚¹
    if privacy_history:
        final_eps = privacy_history[-1]
        plt.annotate(f'Final Îµ = {final_eps:.3f}',
                     xy=(len(privacy_history) - 1, final_eps),
                     xytext=(len(privacy_history) - 1 - len(privacy_history) * 0.2, final_eps * 1.1),
                     arrowprops=dict(arrowstyle='->', color='black'))

    if save_path:
        plt.savefig(save_path, dpi=300, bbox_inches='tight')
        logger.info(f"Privacy budget plot saved to {save_path}")

    plt.show()


def plot_clipping_analysis(norms_before: List[float],
                           norms_after: List[float],
                           clipping_bound: float,
                           save_path: Optional[str] = None):
    """
    ç»˜åˆ¶å‰ªè£åˆ†æå›¾è¡¨

    Args:
        norms_before: å‰ªè£å‰çš„èŒƒæ•°
        norms_after: å‰ªè£åçš„èŒƒæ•°
        clipping_bound: å‰ªè£ç•Œé™
        save_path: ä¿å­˜è·¯å¾„ï¼ˆå¯é€‰ï¼‰
    """
    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))

    # å·¦å›¾ï¼šèŒƒæ•°åˆ†å¸ƒ
    ax1.hist(norms_before, bins=30, alpha=0.7, label='Before Clipping', color='blue')
    ax1.hist(norms_after, bins=30, alpha=0.7, label='After Clipping', color='orange')
    ax1.axvline(x=clipping_bound, color='red', linestyle='--',
                label=f'Clipping Bound = {clipping_bound}')
    ax1.set_xlabel('L2 Norm')
    ax1.set_ylabel('Frequency')
    ax1.set_title('Distribution of Update Norms')
    ax1.legend()
    ax1.grid(True, alpha=0.3)

    # å³å›¾ï¼šæ—¶é—´åºåˆ—
    rounds = list(range(len(norms_before)))
    ax2.plot(rounds, norms_before, 'b-', alpha=0.7, label='Before Clipping')
    ax2.plot(rounds, norms_after, 'o-', alpha=0.7, label='After Clipping', markersize=3)
    ax2.axhline(y=clipping_bound, color='red', linestyle='--',
                label=f'Clipping Bound = {clipping_bound}')
    ax2.set_xlabel('Training Rounds')
    ax2.set_ylabel('L2 Norm')
    ax2.set_title('Update Norms Over Time')
    ax2.legend()
    ax2.grid(True, alpha=0.3)

    plt.tight_layout()

    if save_path:
        plt.savefig(save_path, dpi=300, bbox_inches='tight')
        logger.info(f"Clipping analysis plot saved to {save_path}")

    plt.show()


def plot_data_distribution(client_data_indices: List[List[int]],
                           labels: Union[List, np.ndarray, torch.Tensor],
                           save_path: Optional[str] = None):
    """
    ç»˜åˆ¶å®¢æˆ·ç«¯æ•°æ®åˆ†å¸ƒå›¾

    Args:
        client_data_indices: æ¯ä¸ªå®¢æˆ·ç«¯çš„æ•°æ®ç´¢å¼•
        labels: æ•°æ®æ ‡ç­¾
        save_path: ä¿å­˜è·¯å¾„ï¼ˆå¯é€‰ï¼‰
    """
    if isinstance(labels, torch.Tensor):
        labels = labels.numpy()
    elif isinstance(labels, list):
        labels = np.array(labels)

    num_clients = len(client_data_indices)
    unique_labels = np.unique(labels)
    num_classes = len(unique_labels)

    # è®¡ç®—æ¯ä¸ªå®¢æˆ·ç«¯çš„æ ‡ç­¾åˆ†å¸ƒ
    distribution_matrix = np.zeros((num_clients, num_classes))

    for i, client_indices in enumerate(client_data_indices):
        if len(client_indices) > 0:
            client_labels = labels[client_indices]
            label_counts = np.bincount(client_labels, minlength=num_classes)
            distribution_matrix[i] = label_counts / len(client_indices)

    # åˆ›å»ºçƒ­åŠ›å›¾
    plt.figure(figsize=(12, 8))
    sns.heatmap(distribution_matrix,
                annot=True,
                fmt='.2f',
                cmap='Blues',
                xticklabels=[f'Class {i}' for i in unique_labels],
                yticklabels=[f'Client {i}' for i in range(num_clients)])

    plt.title('Data Distribution Across Clients')
    plt.xlabel('Classes')
    plt.ylabel('Clients')

    if save_path:
        plt.savefig(save_path, dpi=300, bbox_inches='tight')
        logger.info(f"Data distribution plot saved to {save_path}")

    plt.show()


# ================================ é…ç½®å’Œæ—¥å¿—å·¥å…· ================================

def save_dp_config(dp_config: DPConfig, save_path: str):
    """ä¿å­˜DPé…ç½®åˆ°æ–‡ä»¶"""
    config_dict = asdict(dp_config)

    with open(save_path, 'w') as f:
        json.dump(config_dict, f, indent=2)

    logger.info(f"DP config saved to {save_path}")


def load_dp_config(config_path: str) -> DPConfig:
    """ä»æ–‡ä»¶åŠ è½½DPé…ç½®"""
    with open(config_path, 'r') as f:
        config_dict = json.load(f)

    return DPConfig(**config_dict)


def setup_dp_logging(log_level: str = "INFO",
                     log_file: Optional[str] = None) -> logging.Logger:
    """è®¾ç½®DPä¸“ç”¨æ—¥å¿—"""
    dp_logger = logging.getLogger('fed_pruner.dp')
    dp_logger.setLevel(getattr(logging, log_level.upper()))

    # æ ¼å¼åŒ–å™¨
    formatter = logging.Formatter(
        '%(asctime)s - %(name)s - %(levelname)s - %(message)s'
    )

    # æ§åˆ¶å°å¤„ç†å™¨
    console_handler = logging.StreamHandler()
    console_handler.setFormatter(formatter)
    dp_logger.addHandler(console_handler)

    # æ–‡ä»¶å¤„ç†å™¨ï¼ˆå¦‚æœæŒ‡å®šï¼‰
    if log_file:
        file_handler = logging.FileHandler(log_file)
        file_handler.setFormatter(formatter)
        dp_logger.addHandler(file_handler)

    return dp_logger


def create_dp_experiment_summary(dp_manager: DifferentialPrivacyManager,
                                 final_accuracy: float,
                                 baseline_accuracy: float,
                                 training_time: float) -> Dict[str, Any]:
    """åˆ›å»ºDPå®éªŒæ€»ç»“"""
    stats = dp_manager.get_statistics()
    final_eps, delta = dp_manager.get_privacy_budget()

    summary = {
        'experiment_config': {
            'target_epsilon': dp_manager.config.target_epsilon,
            'target_delta': dp_manager.config.target_delta,
            'noise_multiplier': dp_manager.config.noise_multiplier,
            'clipping_bound': dp_manager.config.clipping_bound,
            'num_clients': dp_manager.config.num_clients,
            'num_epochs': dp_manager.config.num_epochs
        },
        'privacy_results': {
            'final_epsilon': final_eps,
            'delta': delta,
            'budget_utilization': final_eps / dp_manager.config.target_epsilon,
            'total_steps': stats['total_steps']
        },
        'performance_results': {
            'final_accuracy': final_accuracy,
            'baseline_accuracy': baseline_accuracy,
            'accuracy_drop': baseline_accuracy - final_accuracy,
            'relative_accuracy_drop': (baseline_accuracy - final_accuracy) / baseline_accuracy,
            'training_time_seconds': training_time
        },
        'clipping_statistics': {
            'total_clipped_clients': stats['total_clipped_clients'],
            'clipping_rate': stats['clipping_rate'],
            'avg_norm_reduction': stats['avg_norm_reduction']
        }
    }

    return summary


# ================================ å®éªŒè¾…åŠ©å·¥å…· ================================

def generate_dp_experiment_configs(base_config: DPConfig,
                                   epsilon_values: List[float],
                                   noise_multipliers: List[float]) -> List[DPConfig]:
    """ç”Ÿæˆä¸€ç³»åˆ—DPå®éªŒé…ç½®"""
    configs = []

    for eps in epsilon_values:
        for noise_mult in noise_multipliers:
            config = deepcopy(base_config)
            config.target_epsilon = eps
            config.noise_multiplier = noise_mult
            configs.append(config)

    logger.info(f"Generated {len(configs)} experiment configurations")
    return configs


def estimate_training_time(dp_config: DPConfig,
                           time_per_round: float) -> Dict[str, float]:
    """ä¼°ç®—è®­ç»ƒæ—¶é—´"""
    total_rounds = dp_config.num_epochs
    estimated_time = total_rounds * time_per_round

    return {
        'total_rounds': total_rounds,
        'time_per_round_seconds': time_per_round,
        'estimated_total_time_seconds': estimated_time,
        'estimated_total_time_hours': estimated_time / 3600
    }


# å¯¼å‡ºä¸»è¦å‡½æ•°
__all__ = [
    'validate_dp_implementation',
    'check_privacy_budget_consumption',
    'analyze_clipping_statistics',
    'compute_privacy_utility_curve',
    'create_non_iid_data_split',
    'compute_data_heterogeneity_metrics',
    'plot_privacy_budget_over_time',
    'plot_clipping_analysis',
    'plot_data_distribution',
    'save_dp_config',
    'load_dp_config',
    'setup_dp_logging',
    'create_dp_experiment_summary',
    'generate_dp_experiment_configs',
    'estimate_training_time'
]
